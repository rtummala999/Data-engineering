
JOINS
======



Joins With 2 files are like we need to execute like this with 2 files 


file 1 > 9 partitions 

n1               n2         n3      n4
p1,p2       p3,p4       p5,p6      p7,p8,p9 

file 2 1mb 2 partitions 

n1  
p1,p2

then we have key suppose 102 

then we need to bring from all nodes and merge in one node 

this is how it works wide transformations 


code for the joins first make sure we mapped the data 


orders=spark.sparkContext.textFile("/public/trendytech/orders/orders_1gb.csv")

orders_map= rdd.map(lambda x: (x.split(",")[2],x.split(",")[3]))
map_rdd.take(5)

    [('11599', 'CLOSED'),
   ('256', 'PENDING_PAYMENT'),
   ('12111', 'COMPLETE'),
   ('8827', 'CLOSED'),
   ('11318', 'COMPLETE')]
customers=spark.sparkContext.textFile("/public/trendytech/retail_db/customers/part-00000")


customers=spark.sparkContext.textFile("/public/trendytech/retail_db/customers/part-00000")
customer_map=customers.map(lambda x: (x.split(",")[0],x.split(",")[8]))

join_rdd=customer_map.join(orders_map)

join_rdd.saveAsTextFile("data/orders_joined1")
